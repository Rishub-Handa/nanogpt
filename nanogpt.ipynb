{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# https://www.youtube.com/watch?v=kCc8FmEb1nY - 1:19:12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "block_size = 256 # context window size for prediction \n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
        "print(\"device\", device)\n",
        "n_embed = 384\n",
        "n_head = 6 # 384 / 6 = 64 dim head \n",
        "n_layer = 6\n",
        "dropout = 0.2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlmxYdX4mdTe"
      },
      "source": [
        "# Get Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTHhNHR0k93z",
        "outputId": "a9ef5028-abb7-43f0-c26f-9ac7ca91664a"
      },
      "outputs": [],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rni3Ojw4lY3E",
        "outputId": "d78ecb5e-8e32-4e30-9064-29e99d4aa282"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f: \n",
        "    text = f.read()\n",
        "print(\"characters\", len(text))\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtN8TDESmhVS"
      },
      "source": [
        "# Tokenize Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMsEi7W0mNNH",
        "outputId": "eab88820-c40f-4e7e-d71f-824893bc09f6"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lTKFT3HmU6x",
        "outputId": "fc7cf70d-95b1-4dc3-81fe-a2939b651ccf"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars )}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(\"encoded\", encode(\"hello world\"))\n",
        "print(\"decoded\", decode(encode(\"hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLFSKou0m67w",
        "outputId": "5781a1cf-3fee-4d83-8b00-54cdde2b565f"
      },
      "outputs": [],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSeT8ONBmb_9"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5frvSPRmmb9G"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "torch.manual_seed(42)\n",
        "\n",
        "def get_batch(split): \n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    s = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in s])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in s])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print(xb.shape, yb.shape)\n",
        "print(xb)\n",
        "print(yb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Head(nn.Module): # Added in later\n",
        "\n",
        "    '''\n",
        "    One head of self-attention \n",
        "    '''\n",
        "\n",
        "    def __init__(self, head_size): \n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "        # This is a buffer, not a parameters\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x): \n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x) # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        # Compute self attention scores (\"affinities\")\n",
        "        w = q @ k.transpose(-2, -1) * C**-0.5# (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = self.dropout(w) # Drop out some of the communication between tokens \n",
        "\n",
        "        out = w @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class MultiHeadAttention(nn.Module): \n",
        "    '''\n",
        "    Multiple self-attention heads in parallel \n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_heads, head_size): \n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Increase the number of channels that a token can use for information with multiple heads \n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n",
        "\n",
        "        # Project back to the original number of channels - this allows us to create the residual pathway in the transformer block \n",
        "        out = self.proj(out) # (B, T, n_embed)\n",
        "\n",
        "        # Dropout for regularization - prevent overfitting\n",
        "        # During training, require the model to learn with a subset of weights (but a different subset with each pass)\n",
        "        # During inference, it's like using an ensemble of sub-networks \n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "    \n",
        "\n",
        "class FeedForward(nn.Module): \n",
        "\n",
        "    def __init__(self, n_embed): \n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4*n_embed),\n",
        "            nn.ReLU(), # Introduct some non-linearity learnings \n",
        "            nn.Linear(4*n_embed, n_embed), # Projection back into residual pathway \n",
        "            nn.Dropout(dropout) # Dropout for regularization - prevent overfitting \n",
        "        )\n",
        "\n",
        "    def forward(self, x): \n",
        "        return self.net(x)\n",
        "    \n",
        "class LayerNorm:\n",
        "    '''\n",
        "    Layer normalization is another technique to help with training deep networks. \n",
        "    It guarantees the output of the layer has zero mean and unit variance (following a normal distribution). \n",
        "    Actual layer norm is more complicated. \n",
        "    '''\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5): \n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "    def __call__(self, x): \n",
        "        xmean = x.mean(1, keepdim=True) # Row mean per batch \n",
        "        xvar = x.var(1, keepdim=True) # Row variance per batch\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # Normalization \n",
        "        self.out = self.gamma * xhat + self.beta # Scale and shift (learned parameters)\n",
        "        return self.out\n",
        "    \n",
        "    def parameters(self): \n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module): \n",
        "    '''\n",
        "    Transformer block: communication, then computation (self-attention, then feed-forward)\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_embed=8, n_head=4): \n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # shape is 8 x 32 (4 parallel heads 8 dim w@v otuputs each)\n",
        "        self.ff = FeedForward(n_embed) # Need some computation time to contemplate the self-attention output before getting to the logits \n",
        "        self.ln1 = nn.LayerNorm(n_embed) # Normalize the input to the self-attention layer - using pytorch LayerNorm \n",
        "        self.ln2 = nn.LayerNorm(n_embed) # Normalize the input to the feed-forward layer - using pytorch LayerNorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The \"x +=\" is a skip connection, which is useful to propogate gradients through the network\n",
        "        # This is necessary for deep neural nets \n",
        "        # At the start of training, the self.sa(x) and self.ff(x) will contribute little, so the skip connection will quickly propogate gradients back \n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNQMmA7Qmb5e"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module): \n",
        "    '''\n",
        "    Description: BLM model that predicts the next token given the current token. \n",
        "    '''\n",
        "\n",
        "    def __init__(self, vocab_size): \n",
        "        super().__init__()\n",
        "\n",
        "        # Encode the identity of a categorical token deterministically to a dense vector space in n_embed dimensions \n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # shape is 65 x 32\n",
        "\n",
        "        # Encode the identity of a token position to n_embed dense vector space \n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed) # shape is 8 x 32\n",
        "\n",
        "        # Transformer block \n",
        "        # Having a lot of blocks makes the net very deep - deep nets are hard to optimize \n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # Normalize the input to the final layer \n",
        "\n",
        "        # Project the n_embed dimensional token embedding to the vocab size to get logits for output char\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size) # shape is 32 x 65\n",
        "    \n",
        "    def forward(self, idx, targets=None): \n",
        "        B, T = idx.shape\n",
        "\n",
        "        # For a given input token, get the corresponding row of the embedding matrix\n",
        "        # This row is the logits (likelihood score) for the next token \n",
        "        # The parameters to pick the right logits gets trained during backprop\n",
        "        token_emb = self.token_embedding_table(idx) # Output is 4 x 8 x 32\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # Output is 8 x 32\n",
        "        x = token_emb + pos_emb # Output is 4 x 8 x 32\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # Output is 4 x 8 x 65\n",
        "        \n",
        "        if targets is None: \n",
        "            return logits, None\n",
        "        \n",
        "        # Use negative log likelihood loss to train the model\n",
        "        # b = batch, t = block, c = vocab size\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C) # Output is 32 x 65\n",
        "        targets = targets.view(B*T) # Output is 32\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens): \n",
        "\n",
        "        # idx is (B, T)\n",
        "        for _ in range(max_new_tokens): \n",
        "            \n",
        "            # trim idx to the last block size tokens \n",
        "            idx_cond = idx[:, -block_size:] # (B, T)\n",
        "\n",
        "            logits, loss = self(idx_cond)\n",
        "\n",
        "            # Only care about the last time step\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Get probabilities from logits \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from distribution \n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append sampled token to sequence \n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "        return idx #(B, T+1)\n",
        "\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape, loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_iters = 100\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA5ZkNwJmb3b"
      },
      "outputs": [],
      "source": [
        "print(\"We expect the loss to be around\", -np.log(1/65))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # 0 is the \\n character so good to start off\n",
        "print(decode(m.generate(context, 100).cpu().numpy()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimizer \n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "batch_size = 32\n",
        "for steps in range(10000): \n",
        "\n",
        "    if steps % 1000 == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(\"Step\", steps, \"Train loss\", losses[\"train\"], \"Val loss\", losses[\"val\"])\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(decode(m.generate(context, 100).cpu().numpy()[0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math trick in self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "In bigram, we are only looking at the previous token to predict the next token. It has no context. \n",
        "To introduce context, we can naively take the average of all the previous tokens to give some information about what came before. \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We want xbow[b, t] = mean_{i<=t} x[b, i]\n",
        "# X Bag of Words - term for averaging \n",
        "xbow = torch.zeros((B, T, C))\n",
        "# For each batch\n",
        "for b in range(B): \n",
        "    # For each token in the block \n",
        "    for t in range(T): \n",
        "        # Get the mean of all the previous tokens\n",
        "        xprev = x[b, :t+1] # (t, C)\n",
        "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
        "xbow[0], x[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This process can be sped up w matrix multiplication\n",
        "w = torch.tril(torch.ones((T, T))) # Lower triangular matrix\n",
        "w = w / torch.sum(w, dim=1, keepdim=True) # Normalize\n",
        "\n",
        "# With the lower tri matrix, we can do matrix multiplication to get the same result\n",
        "# For each row i in the matmul, it takes the mean of the first i rows of x\n",
        "# This matrix multiplication gives us context about the previous tokens by taking their weight sum (mean in this naive case)\n",
        "print(w)\n",
        "xbow2 = w @ x # (B, T, T) @ (B, T, C) = (B, T, C)\n",
        "print(xbow2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another way to do this \n",
        "tril = torch.tril(torch.ones((T, T)))\n",
        "w = torch.zeros((T, T)) # These are our attenion weights (how much we care about each previous token) \n",
        "                        # -> allow for a weighted sum of past elements \n",
        "\n",
        "w = w.masked_fill(tril == 0, float('-inf')) # Replace 0 with -inf -> don't peek into the future (represented by upper tri)\n",
        "\n",
        "w = F.softmax(w, dim=1) # Normalize -> gives us the weights for each token \n",
        "                        # (same matrix as above; weights are equal for each token since we're taking the mean)\n",
        "                        # Normalizes weights to sum to 1\n",
        "print(w)\n",
        "xbow3 = w @ x # (B, T, T) @ (B, T, C) = (B, T, C)\n",
        "print(xbow3[0])\n",
        "print(torch.allclose(xbow2, xbow3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Self attention \n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 32 # increased channels to 32 -> each token has a 32 dim vector representation\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# example of single Head perform self-attention\n",
        "head_size = 16\n",
        "\n",
        "# Keys and querys are produced independently from the channels \n",
        "key = nn.Linear(C, head_size, bias=False) # 16x32\n",
        "query = nn.Linear(C, head_size, bias=False) # 16x32\n",
        "value = nn.Linear(C, head_size, bias=False) # 16x32\n",
        "\n",
        "# Every token has a query and key, generated by its identity + positional encoding \n",
        "# query - what am i looking for? i.e. i'm looking for previous vowels \n",
        "# key - what do i contain? i.e. i'm a constant in the beginning of the block \n",
        "# value - the thing that gets weighted sum and moves on in the calculation \n",
        "# If query/key are aligned, then we will have a high dot product \n",
        "k = key(x) # (B, T, head_size)\n",
        "q = query(x) # (B, T, head_size)\n",
        "v = value(x) # (B, T, head_size) \n",
        "\n",
        "w = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)\n",
        "w = w / np.sqrt(head_size) # Normalize -> want to make numbers smaller so softmax doesn't sharpen to 0 or 1 (becomes OHE)\n",
        "tril = torch.tril(torch.ones((T, T)))\n",
        "w = w.masked_fill(tril == 0, float('-inf')) # Replace 0 with -inf -> don't peek into the future (represented by upper tri)\n",
        "                                            # In some cases, you might not want to mask the future (i.e. protein language modeling)\n",
        "w = F.softmax(w, dim=-1) # Normalize -> gives us the weights for each token\n",
        "\n",
        "# For each token, the output channels is a weighted sum of prev tokens \"values\"\n",
        "# The weight applied to each token-value-vector is determined by the dot product of the query and key\n",
        "# The \"out\" is then propogated to the next layer \n",
        "out = w @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
        "\n",
        "# We might have multiple heads, so for each x, you might have mulitple v's to communicate further in the network \n",
        "#   Each v is kind of like the feature vector of a token to communicate to the next layer\n",
        "#   To know which v's are important, you need alignment between the query and key\n",
        "print(out.shape)\n",
        "print(out[0])\n",
        "print(w[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remember the upper tri is masked \n",
        "# in the lower tri position (8, 7) was created by mulitplying the query of token 8 with the key of token 7\n",
        "#   -> 8 broadcasts what it's looking for (in head_size dim vector repreesntation), and 7 broadcasts what it contains (in some embedding representation space of head_size)\n",
        "#   -> high score means high dot product, means high alignment \n",
        "(q @ k.transpose(-2, -1))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The above is called self-attention because q,k, and v all come from the same x\n",
        "# In cross attention, the query comes from x and k/v come from y \n",
        "#   Basically, x is \"looking\" for some characteristic in tokens of y. \n",
        "#   Y's \"keys\" project \"what they are\". If there is alignment, those y-values are propogated "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
